Type	Front	Back	Tags
Basic	What is the "no free lunch theorem"?	No classifier is best for all scenarios	chapter03::algorithm-selection
Basic	What does "no free lunch" imply?	Algorithm choice depends on the data	chapter03::algorithm-selection
Basic	What is a "hyperplane"?	Decision boundary in high-dimensional space	chapter03::prerequisites
Basic	What are "odds"?	Ratio of success to failure: p/(1-p)	chapter03::prerequisites
Basic	What is a "log" (logarithm)?	Inverse of exponentiation	chapter03::prerequisites
Basic	What is "probability"?	Likelihood of an event, between 0 and 1	chapter03::prerequisites
Basic	What is a "kernel" (in ML)?	Function measuring similarity between points	chapter03::prerequisites
Basic	What is "entropy" (in information theory)?	Measure of uncertainty/randomness	chapter03::prerequisites
Basic	What is "impurity" in decision trees?	How mixed the classes are in a node	chapter03::prerequisites
Basic	Step 1 of supervised ML workflow?	Collect labeled training examples	chapter03::ml-workflow
Basic	Step 2 of supervised ML workflow?	Choose a performance metric	chapter03::ml-workflow
Basic	Step 3 of supervised ML workflow?	Choose and train a model	chapter03::ml-workflow
Basic	Step 4 of supervised ML workflow?	Evaluate model performance	chapter03::ml-workflow
Basic	Step 5 of supervised ML workflow?	Tune hyperparameters	chapter03::ml-workflow
Basic	What does stratify do in train_test_split?	Preserves class proportions	chapter03::data-splitting
Basic	What does StandardScaler.fit() do?	Learn mean and std from training data	chapter03::preprocessing
Basic	What does StandardScaler.transform() do?	Apply learned scaling to data	chapter03::preprocessing
Basic	Why use same scaler for train and test?	Keeps values comparable	chapter03::preprocessing
Basic	What is OvR (One-vs-Rest)?	Binary classifier per class vs. all others	chapter03::multiclass
Basic	What is logistic regression used for?	Classification (not regression)	chapter03::logistic-regression
Basic	What does logistic regression output?	Class probabilities	chapter03::logistic-regression
Cloze	Odds formula: {{c1::p / (1-p)}}		chapter03::logistic-regression
Cloze	Logit function: logit(p) = {{c1::log(p/(1-p))}}		chapter03::logistic-regression
Cloze	Sigmoid function: σ(z) = {{c1::1 / (1 + e^(-z))}}		chapter03::logistic-regression
Basic	What is the relationship between logit and sigmoid?	Sigmoid is inverse of logit	chapter03::logistic-regression
Basic	What is σ(z) when z = 0?	0.5	chapter03::logistic-regression
Basic	How does logistic regression differ from Adaline?	Uses sigmoid instead of identity activation	chapter03::logistic-regression
Basic	When is class 1 predicted in logistic regression?	When σ(z) >= 0.5	chapter03::logistic-regression
Basic	Why use logistic regression in medicine?	Outputs interpretable probabilities	chapter03::logistic-regression
Cloze	Logistic loss when y=1: {{c1::-log(σ(z))}}		chapter03::logistic-regression
Cloze	Logistic loss when y=0: {{c1::-log(1 - σ(z))}}		chapter03::logistic-regression
Basic	What is overfitting?	Good training performance, poor generalization	chapter03::overfitting
Basic	What is underfitting?	Model too simple to capture patterns	chapter03::overfitting
Basic	What is regularization?	Penalizing extreme weights to prevent overfitting	chapter03::regularization
Cloze	L2 regularization adds: {{c1::(λ/2) ||w||²}}		chapter03::regularization
Basic	What does C parameter control in sklearn?	Inverse of regularization strength	chapter03::regularization
Basic	What happens when C decreases?	Stronger regularization, smaller weights	chapter03::regularization
Basic	Why scale features for regularization?	Equal penalty across all features	chapter03::regularization
Basic	What is SVM?	Support Vector Machine	chapter03::svm
Basic	What does SVM maximize?	The margin between classes	chapter03::svm
Basic	What are support vectors?	Training points closest to decision boundary	chapter03::svm
Basic	Why do large margins help SVMs?	Lower generalization error	chapter03::svm
Basic	What is soft-margin classification?	SVM allowing some misclassifications	chapter03::svm
Basic	What is a slack variable?	Allows points inside or wrong side of margin	chapter03::svm
Basic	How does C affect SVM?	Large C: tight fit; Small C: wider margin	chapter03::svm
Basic	When prefer logistic regression over SVM?	Need probabilities or interpretability	chapter03::algorithm-comparison
Basic	When prefer SVM over logistic regression?	Care about margin, need kernels	chapter03::algorithm-comparison
Basic	What is the kernel trick?	Implicit mapping to higher dimensions	chapter03::kernel-svm
Basic	What is the RBF kernel?	Gaussian/radial basis function kernel	chapter03::kernel-svm
Cloze	RBF kernel: k(x,x') = exp({{c1::-γ||x-x'||²}})		chapter03::kernel-svm
Basic	What does γ (gamma) control in RBF?	Influence radius of training points	chapter03::kernel-svm
Basic	What does large gamma cause?	Tighter, more complex boundaries	chapter03::kernel-svm
Basic	What does small gamma cause?	Smoother boundaries	chapter03::kernel-svm
Basic	Why are decision trees interpretable?	Can trace decision path	chapter03::decision-trees
Basic	What do decision trees maximize at splits?	Information gain	chapter03::decision-trees
Basic	Name three impurity measures.	Gini, entropy, classification error	chapter03::decision-trees
Cloze	Entropy: I_H = {{c1::-Σp(i)log₂p(i)}}		chapter03::decision-trees
Cloze	Gini impurity: I_G = {{c1::1 - Σp(i)²}}		chapter03::decision-trees
Basic	When is entropy maximal (binary)?	When p = 0.5 for each class	chapter03::decision-trees
Basic	Why not use classification error for splitting?	Less sensitive to probability changes	chapter03::decision-trees
Basic	Do decision trees need feature scaling?	No	chapter03::decision-trees
Basic	What is a random forest?	Ensemble of decision trees	chapter03::random-forest
Basic	Step 1 of random forest algorithm?	Bootstrap sample (sample with replacement)	chapter03::random-forest
Basic	Step 2 of random forest algorithm?	Grow tree using random feature subset	chapter03::random-forest
Basic	Step 3 of random forest algorithm?	Repeat k times	chapter03::random-forest
Basic	Step 4 of random forest algorithm?	Aggregate by majority vote	chapter03::random-forest
Basic	What is bootstrap sampling?	Random sampling with replacement	chapter03::random-forest
Basic	Why use random feature subsets?	Increases diversity among trees	chapter03::random-forest
Cloze	Default features per split: {{c1::√m}} where m = total features		chapter03::random-forest
Basic	Main hyperparameter in random forests?	Number of trees (n_estimators)	chapter03::random-forest
Basic	Do random forests need pruning?	No, averaging handles noise	chapter03::random-forest
Basic	Why is KNN a "lazy learner"?	Memorizes data, computes at query time	chapter03::knn
Basic	Step 1 of KNN algorithm?	Choose k and distance metric	chapter03::knn
Basic	Step 2 of KNN algorithm?	Find k nearest neighbors	chapter03::knn
Basic	Step 3 of KNN algorithm?	Majority vote for class label	chapter03::knn
Basic	What is Minkowski distance?	Generalized distance metric	chapter03::knn
Cloze	Euclidean distance: Minkowski with p={{c1::2}}		chapter03::knn
Cloze	Manhattan distance: Minkowski with p={{c1::1}}		chapter03::knn
Basic	Why standardize features for KNN?	Equal contribution to distance	chapter03::knn
Basic	How does k affect KNN bias-variance?	Small k: low bias, high variance	chapter03::knn
Basic	What is curse of dimensionality?	High dimensions make neighbors far away	chapter03::knn
Basic	What is a parametric model?	Fixed number of parameters	chapter03::ml-concepts
Basic	What is a non-parametric model?	Parameters grow with data size	chapter03::ml-concepts
Basic	Main disadvantage of KNN?	Slow prediction on large datasets	chapter03::knn
Basic	Main advantage of KNN?	Instantly adapts to new data	chapter03::knn
Basic	Write code to split data with stratification.	train_test_split(X, y, test_size=0.3, stratify=y)	chapter03::code
Basic	Write code to create a StandardScaler.	from sklearn.preprocessing import StandardScaler; sc = StandardScaler()	chapter03::code
Basic	Write code to fit and transform training data.	X_train_std = sc.fit_transform(X_train)	chapter03::code
Basic	Write code to transform test data only.	X_test_std = sc.transform(X_test)	chapter03::code
Basic	Write code to create LogisticRegression.	from sklearn.linear_model import LogisticRegression; lr = LogisticRegression()	chapter03::code
Basic	Write code to train a sklearn model.	model.fit(X_train, y_train)	chapter03::code
Basic	Write code to make predictions.	y_pred = model.predict(X_test)	chapter03::code
Basic	Write code to get class probabilities.	proba = model.predict_proba(X_test)	chapter03::code
Basic	Write code to compute accuracy.	from sklearn.metrics import accuracy_score; accuracy_score(y_test, y_pred)	chapter03::code
Basic	Write code to create SVM with RBF kernel.	from sklearn.svm import SVC; svm = SVC(kernel='rbf', gamma=0.1)	chapter03::code
Basic	Write code to create linear SVM.	SVC(kernel='linear', C=1.0)	chapter03::code
Basic	Write code to create DecisionTreeClassifier.	from sklearn.tree import DecisionTreeClassifier; tree = DecisionTreeClassifier(max_depth=4)	chapter03::code
Basic	Write code to create RandomForestClassifier.	from sklearn.ensemble import RandomForestClassifier; rf = RandomForestClassifier(n_estimators=100)	chapter03::code
Basic	Write code to create KNeighborsClassifier.	from sklearn.neighbors import KNeighborsClassifier; knn = KNeighborsClassifier(n_neighbors=5)	chapter03::code
Basic	Write code for SGDClassifier with hinge loss.	from sklearn.linear_model import SGDClassifier; sgd = SGDClassifier(loss='hinge')	chapter03::code
Basic	Write code to set regularization in LogisticRegression.	LogisticRegression(C=0.1)  # lower C = stronger regularization	chapter03::code
Basic	Write code to get feature importances from tree.	tree.feature_importances_	chapter03::code
