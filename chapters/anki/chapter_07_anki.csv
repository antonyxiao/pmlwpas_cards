Type	Front	Back	Tags
Basic	What is an "ensemble" in ML?	Combining multiple models	chapter07::prerequisites
Basic	What is a "weak learner"?	Model slightly better than random	chapter07::prerequisites
Basic	What is "bootstrapping"?	Sampling with replacement	chapter07::prerequisites
Basic	What is a "residual" (in boosting)?	Prediction error	chapter07::prerequisites
Basic	What is ensemble learning's goal?	Better generalization than single models	chapter07::ensemble-basics
Basic	Majority vs plurality voting?	Majority needs >50%; plurality needs most votes	chapter07::voting
Basic	When does ensemble outperform individuals?	Base classifiers are independent with uncorrelated errors	chapter07::ensemble-basics
Basic	What is weighted majority voting?	Votes weighted by classifier importance	chapter07::voting
Basic	What is bagging?	Bootstrap AGGregatING	chapter07::bagging
Cloze	Bagging samples {{c1::with replacement}}		chapter07::bagging
Basic	What does bagging reduce?	Variance	chapter07::bagging
Basic	What doesn't bagging reduce?	Bias	chapter07::bagging
Basic	Best base classifiers for bagging?	Low bias, high variance (e.g., deep trees)	chapter07::bagging
Basic	How does random forest extend bagging?	Adds random feature subsets at splits	chapter07::random-forest
Basic	What is boosting?	Sequential training on misclassified examples	chapter07::boosting
Cloze	Typical boosting weak learner: {{c1::decision stump}} (depth=1)		chapter07::boosting
Basic	What does boosting reduce?	Both bias and variance	chapter07::boosting
Basic	What is AdaBoost?	Adaptive Boosting	chapter07::adaboost
Basic	How does AdaBoost weight samples?	Increases weight for misclassified	chapter07::adaboost
Basic	How are AdaBoost weights initialized?	Uniform (1/n)	chapter07::adaboost
Cloze	AdaBoost alpha = {{c1::0.5 * log((1-ε)/ε)}}		chapter07::adaboost
Basic	When is AdaBoost alpha = 0?	Error rate = 0.5 (random)	chapter07::adaboost
Basic	Final AdaBoost prediction method?	Weighted majority vote	chapter07::adaboost
Basic	When choose bagging?	High variance, need parallelization	chapter07::ensemble-comparison
Basic	When choose boosting?	Weak learners that underfit	chapter07::ensemble-comparison
Basic	What is stacking?	Two-level ensemble with meta-classifier	chapter07::stacking
Basic	Common stacking meta-classifier?	Logistic regression	chapter07::stacking
Basic	What is gradient boosting?	Train trees on prediction residuals	chapter07::gradient-boosting
Cloze	Gradient boosting tree depth: {{c1::3 to 6}}		chapter07::gradient-boosting
Basic	What is the pseudo-residual?	Negative gradient of loss	chapter07::gradient-boosting
Cloze	Gradient boosting learning rate: {{c1::0.01 to 0.1}}		chapter07::gradient-boosting
Basic	Learning rate vs n_estimators tradeoff?	Lower rate needs more trees	chapter07::gradient-boosting
Basic	What is XGBoost?	Extreme Gradient Boosting	chapter07::xgboost
Basic	Other gradient boosting libraries?	LightGBM, CatBoost	chapter07::gradient-boosting
Basic	Main ensemble tradeoff?	Better accuracy vs more complexity	chapter07::ensemble-comparison
Basic	Write code to create VotingClassifier.	from sklearn.ensemble import VotingClassifier; vc = VotingClassifier(estimators=[('lr', lr), ('rf', rf)])	chapter07::code
Basic	Soft vs hard voting parameter?	voting='soft' or voting='hard'	chapter07::code
Basic	Write code to create BaggingClassifier.	from sklearn.ensemble import BaggingClassifier; bag = BaggingClassifier(base_estimator=tree, n_estimators=500)	chapter07::code
Basic	Write code to create AdaBoostClassifier.	from sklearn.ensemble import AdaBoostClassifier; ada = AdaBoostClassifier(n_estimators=500, learning_rate=0.1)	chapter07::code
Basic	Write code to create GradientBoostingClassifier.	from sklearn.ensemble import GradientBoostingClassifier; gb = GradientBoostingClassifier(n_estimators=100)	chapter07::code
Basic	Write code to import XGBoost.	from xgboost import XGBClassifier	chapter07::code
Basic	Write code to create XGBClassifier.	xgb = XGBClassifier(n_estimators=100, learning_rate=0.1, max_depth=3)	chapter07::code
Basic	Write code to create StackingClassifier.	from sklearn.ensemble import StackingClassifier	chapter07::code
