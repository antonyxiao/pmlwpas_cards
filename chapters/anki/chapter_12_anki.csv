Type	Front	Back	Tags
Basic	What is a "tensor"?	Multi-dimensional array	chapter12::prerequisites
Basic	What is "autograd"?	Automatic gradient computation	chapter12::prerequisites
Basic	What is a "computation graph"?	Graph of operations for autodiff	chapter12::prerequisites
Basic	What is PyTorch?	Deep learning library by Meta/Facebook	chapter12::pytorch-basics
Basic	Why use GPUs for neural networks?	Many cores for parallel matrix ops	chapter12::gpu-training
Basic	How does PyTorch build computation graph?	Dynamically during execution	chapter12::computation-graph
Cloze	Scalar is rank-{{c1::0}} tensor		chapter12::tensors
Cloze	Vector is rank-{{c1::1}} tensor		chapter12::tensors
Cloze	Matrix is rank-{{c1::2}} tensor		chapter12::tensors
Basic	Two key PyTorch tensor features?	Autograd and GPU support	chapter12::tensors
Basic	What is DataLoader for?	Batching, shuffling, parallel loading	chapter12::data-loading
Basic	Custom Dataset must implement?	__init__, __getitem__, optionally __len__	chapter12::dataset
Basic	What is nn.Module?	Base class for neural networks	chapter12::nn-module
Basic	Custom nn.Module must define?	__init__ and forward	chapter12::nn-module
Basic	What is nn.Linear?	Fully connected layer	chapter12::nn-module
Basic	Why call optimizer.zero_grad()?	Clear accumulated gradients	chapter12::training-loop
Basic	PyTorch training step order?	forward → loss → backward → step → zero_grad	chapter12::training-loop
Basic	What does loss.backward() do?	Computes gradients	chapter12::autograd
Basic	What does optimizer.step() do?	Updates parameters	chapter12::training-loop
Basic	Model file extensions in PyTorch?	.pt or .pth	chapter12::model-saving
Basic	What does model.eval() do?	Sets evaluation mode (disables dropout)	chapter12::model-evaluation
Cloze	Sigmoid range: {{c1::(0, 1)}}		chapter12::activation-functions
Cloze	Tanh range: {{c1::(-1, 1)}}		chapter12::activation-functions
Cloze	ReLU: {{c1::max(0, z)}}		chapter12::activation-functions
Basic	Why does ReLU help gradients?	Derivative is 1 for positive inputs	chapter12::activation-functions
Basic	What is softmax for?	Convert logits to probabilities summing to 1	chapter12::activation-functions
Cloze	Softmax: {{c1::e^zᵢ / Σe^zⱼ}}		chapter12::activation-functions
Basic	Why standardize features?	Faster, more reliable convergence	chapter12::preprocessing
Basic	Write code to create tensor from list.	torch.tensor([1, 2, 3])	chapter12::code
Basic	Write code to create tensor from numpy.	torch.from_numpy(array)	chapter12::code
Basic	Write code to create zeros tensor.	torch.zeros(2, 3)	chapter12::code
Basic	Write code to create ones tensor.	torch.ones(2, 3)	chapter12::code
Basic	Write code to create random tensor.	torch.rand(2, 3)	chapter12::code
Basic	Write code to set random seed.	torch.manual_seed(42)	chapter12::code
Basic	Write code for matrix multiply.	torch.matmul(A, B)	chapter12::code
Basic	Write code to reshape tensor.	tensor.reshape(2, 3)	chapter12::code
Basic	Write code to transpose tensor.	torch.transpose(t, 0, 1)	chapter12::code
Basic	Write code to create DataLoader.	DataLoader(dataset, batch_size=32, shuffle=True)	chapter12::code
Basic	Write code to create TensorDataset.	TensorDataset(X_tensor, y_tensor)	chapter12::code
Basic	Write code to create Linear layer.	nn.Linear(in_features, out_features)	chapter12::code
Basic	Write code for MSE loss.	nn.MSELoss()	chapter12::code
Basic	Write code for CrossEntropy loss.	nn.CrossEntropyLoss()	chapter12::code
Basic	Write code to create SGD optimizer.	torch.optim.SGD(model.parameters(), lr=0.01)	chapter12::code
Basic	Write code to create Adam optimizer.	torch.optim.Adam(model.parameters(), lr=0.001)	chapter12::code
Basic	Write code to get predicted classes.	torch.argmax(output, dim=1)	chapter12::code
Basic	Write code to save model.	torch.save(model.state_dict(), 'model.pt')	chapter12::code
Basic	Write code to load model.	model.load_state_dict(torch.load('model.pt'))	chapter12::code
Basic	Write code to disable gradients.	with torch.no_grad():	chapter12::code
Basic	Write code to enable gradients on tensor.	tensor.requires_grad_(True)	chapter12::code
