Type	Front	Back	Tags
Basic	What is a "neural network"?	Interconnected layers of nodes (neurons)	chapter11::prerequisites
Basic	What is "backpropagation"?	Algorithm to compute gradients layer-by-layer	chapter11::prerequisites
Basic	What is an "activation function"?	Nonlinear function applied to layer output	chapter11::prerequisites
Basic	What is a "hidden layer"?	Layer between input and output	chapter11::prerequisites
Basic	What is "forward propagation"?	Passing data through network input to output	chapter11::prerequisites
Basic	What is deep learning?	ML with multi-layer neural networks	chapter11::deep-learning-basics
Basic	What is an MLP?	Multilayer Perceptron	chapter11::mlp-architecture
Basic	What makes MLP "multilayer"?	At least one hidden layer	chapter11::mlp-architecture
Basic	What is "feedforward"?	Data flows one direction, no loops	chapter11::feedforward
Cloze	Net input: z = {{c1::wᵀx + b}}		chapter11::net-input
Basic	Why use nonlinear activations?	Without them, stacked layers equal one linear layer	chapter11::activation-functions
Cloze	Sigmoid: σ(z) = {{c1::1 / (1 + e⁻ᶻ)}}		chapter11::activation-functions
Cloze	Sigmoid output range: {{c1::(0, 1)}}		chapter11::activation-functions
Basic	What is vanishing gradient problem?	Gradients become tiny in early layers	chapter11::vanishing-gradients
Basic	Why does sigmoid cause vanishing gradients?	Max derivative is only 0.25	chapter11::vanishing-gradients
Cloze	Sigmoid derivative: {{c1::σ(z)(1 - σ(z))}}		chapter11::sigmoid-derivative
Basic	MLP training step 1?	Forward propagate to get output	chapter11::training-procedure
Basic	MLP training step 2?	Compute loss	chapter11::training-procedure
Basic	MLP training step 3?	Backpropagate and update weights	chapter11::training-procedure
Basic	What is one-hot encoding for NNs?	Binary vector with 1 at class index	chapter11::one-hot-encoding
Cloze	One-hot for class 1 of 3: {{c1::[0,1,0]}}		chapter11::one-hot-encoding
Basic	Why initialize weights to small random values?	Break symmetry, avoid saturation	chapter11::weight-initialization
Basic	How are biases typically initialized?	Zeros	chapter11::bias-initialization
Basic	What is the bias unit for?	Shifts activation function	chapter11::bias-units
Cloze	MNIST: {{c1::70,000}} images, {{c2::28×28}} pixels		chapter11::mnist-dataset
Basic	Why normalize pixels to [-1, 1]?	More stable optimization	chapter11::data-preprocessing
Basic	What is mini-batch SGD?	Update weights using small subsets	chapter11::mini-batch-learning
Basic	Mini-batch advantage 1?	More frequent updates	chapter11::mini-batch-learning
Basic	Mini-batch advantage 2?	Noise helps escape local minima	chapter11::mini-batch-learning
Basic	What is the chain rule?	d/dx[f(g(x))] = (df/dg)(dg/dx)	chapter11::chain-rule
Basic	How to detect overfitting during training?	Gap between train and validation accuracy	chapter11::overfitting
Basic	Two techniques to reduce NN overfitting?	L2 regularization, dropout	chapter11::regularization
Basic	What is L2 regularization in NNs?	Add λ × sum(w²) to loss	chapter11::l2-regularization
Basic	What is dropout?	Randomly zero out neurons during training	chapter11::dropout
Basic	How does dropout prevent overfitting?	Forces redundant representations	chapter11::dropout
Basic	How to get predicted class from NN output?	argmax	chapter11::prediction
Basic	Learning rate too high causes?	Overshooting, oscillating loss	chapter11::learning-rate
Basic	Learning rate too low causes?	Very slow training	chapter11::learning-rate
Basic	What are skip connections?	Shortcuts allowing gradients to bypass layers	chapter11::skip-connections
Basic	What is an epoch?	One pass through entire training set	chapter11::training-terminology
Basic	Why use a validation set?	Detect overfitting, tune hyperparameters	chapter11::validation-set
Basic	Why are GPUs important for NNs?	Parallel matrix operations, 10-100x speedup	chapter11::gpu-computing
Cloze	{{c1::Cross-entropy}} is preferred over MSE for classification		chapter11::cross-entropy
Basic	Write code to one-hot encode labels.	np.eye(num_classes)[labels]	chapter11::code
Basic	Write code to compute softmax.	np.exp(z) / np.sum(np.exp(z), axis=1, keepdims=True)	chapter11::code
Basic	Write code to compute cross-entropy loss.	-np.sum(y_onehot * np.log(y_pred + 1e-7)) / n	chapter11::code
Basic	Write code for sigmoid function.	def sigmoid(z): return 1 / (1 + np.exp(-z))	chapter11::code
Basic	Write code for sigmoid derivative.	sigmoid(z) * (1 - sigmoid(z))	chapter11::code
