Type	Front	Back	Tags
Basic	What is "NLP"?	Natural Language Processing	chapter08::prerequisites
Basic	What is "sentiment"?	Opinion or attitude in text	chapter08::prerequisites
Basic	What is a "corpus"?	Collection of text documents	chapter08::prerequisites
Basic	What is a "token"?	Individual word or unit of text	chapter08::prerequisites
Basic	What is a "vocabulary"?	Set of unique words in corpus	chapter08::prerequisites
Basic	What is "sparse" data?	Mostly zeros	chapter08::prerequisites
Basic	What is sentiment analysis?	Analyzing opinion/attitude in text	chapter08::sentiment-analysis
Basic	What is bag-of-words model?	Word occurrence counts as features	chapter08::bag-of-words
Basic	Why are BoW vectors sparse?	Most words don't appear in each doc	chapter08::bag-of-words
Basic	BoW limitation?	Ignores word order	chapter08::bag-of-words
Basic	What is term frequency (tf)?	Count of word in document	chapter08::term-frequency
Basic	What is TF-IDF?	Term Frequency Ã— Inverse Document Frequency	chapter08::tfidf
Basic	What problem does TF-IDF solve?	Downweights common words	chapter08::tfidf
Cloze	IDF = log({{c1::n_docs / (1 + df)}})		chapter08::tfidf
Basic	Why log in IDF formula?	Prevents rare words being over-weighted	chapter08::tfidf
Basic	What is L2 normalization in TF-IDF?	Scale vectors to unit length	chapter08::tfidf
Basic	Text preprocessing step 1?	Remove HTML markup	chapter08::preprocessing
Basic	Text preprocessing step 2?	Handle emoticons	chapter08::preprocessing
Basic	Text preprocessing step 3?	Remove punctuation	chapter08::preprocessing
Basic	Text preprocessing step 4?	Convert to lowercase	chapter08::preprocessing
Basic	Text preprocessing step 5?	Remove stop words (optional)	chapter08::preprocessing
Basic	Why preserve emoticons?	Carry strong sentiment signals	chapter08::preprocessing
Basic	What are stop words?	Common words like "the", "is", "and"	chapter08::preprocessing
Basic	When is stop word removal most useful?	With raw term frequencies (not TF-IDF)	chapter08::preprocessing
Basic	What is tokenization?	Splitting text into individual words	chapter08::tokenization
Basic	What is stemming?	Reducing words to root form	chapter08::stemming
Basic	What is the Porter stemmer?	Classic suffix-stripping algorithm	chapter08::stemming
Basic	Stemming vs lemmatization?	Stemming is crude; lemmatization is grammatically correct	chapter08::stemming
Basic	What is a unigram?	Single word (1-gram)	chapter08::ngrams
Basic	What is an n-gram?	Sequence of n words	chapter08::ngrams
Basic	Why use n-grams?	Capture word context and phrases	chapter08::ngrams
Basic	What is out-of-core learning?	Training on data too large for memory	chapter08::out-of-core
Basic	How does out-of-core work?	Process mini-batches incrementally	chapter08::out-of-core
Basic	Why can't CountVectorizer do out-of-core?	Needs full vocabulary in memory	chapter08::out-of-core
Basic	What is the hashing trick?	Hash words directly to indices	chapter08::out-of-core
Basic	Hashing trick tradeoff?	More features = fewer collisions but more memory	chapter08::out-of-core
Basic	What is LDA (topic modeling)?	Latent Dirichlet Allocation	chapter08::topic-modeling
Basic	LDA input format?	Word counts (not TF-IDF)	chapter08::topic-modeling
Basic	What does LDA output?	Doc-topic and word-topic matrices	chapter08::topic-modeling
Basic	What is word2vec?	Algorithm for word embeddings	chapter08::embeddings
Basic	What are word embeddings?	Dense vectors capturing word meaning	chapter08::embeddings
Basic	Write code to create CountVectorizer.	from sklearn.feature_extraction.text import CountVectorizer; cv = CountVectorizer()	chapter08::code
Basic	Write code to fit and transform text.	X = cv.fit_transform(docs)	chapter08::code
Basic	How to get vocabulary mapping?	cv.vocabulary_	chapter08::code
Basic	Write code to create TfidfVectorizer.	from sklearn.feature_extraction.text import TfidfVectorizer; tfidf = TfidfVectorizer()	chapter08::code
Basic	Write code to create TfidfTransformer.	from sklearn.feature_extraction.text import TfidfTransformer	chapter08::code
Basic	Write code to set n-gram range.	CountVectorizer(ngram_range=(1, 2))  # unigrams and bigrams	chapter08::code
Basic	Write code to create HashingVectorizer.	from sklearn.feature_extraction.text import HashingVectorizer; hv = HashingVectorizer(n_features=2**16)	chapter08::code
Basic	Write code for SGDClassifier partial_fit.	clf.partial_fit(X_batch, y_batch, classes=[0, 1])	chapter08::code
Basic	Write code to remove HTML with regex.	import re; text = re.sub('<[^>]*>', '', text)	chapter08::code
Basic	Write code to import NLTK stopwords.	from nltk.corpus import stopwords; stop = stopwords.words('english')	chapter08::code
Basic	Write code for Porter stemmer.	from nltk.stem.porter import PorterStemmer; porter = PorterStemmer(); porter.stem(word)	chapter08::code
Basic	Write code to create LDA.	from sklearn.decomposition import LatentDirichletAllocation; lda = LatentDirichletAllocation(n_components=10)	chapter08::code
