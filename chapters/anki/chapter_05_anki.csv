Type	Front	Back	Tags
Basic	What is an "eigenvalue"?	Scalar showing how much eigenvector is stretched	chapter05::prerequisites
Basic	What is an "eigenvector"?	Vector unchanged in direction by transformation	chapter05::prerequisites
Basic	What is a "covariance matrix"?	Matrix of pairwise feature covariances	chapter05::prerequisites
Basic	What is "variance"?	Spread of data around the mean	chapter05::prerequisites
Basic	What is a "manifold"?	Lower-dimensional surface in higher-dimensional space	chapter05::prerequisites
Basic	What is "orthogonal"?	Perpendicular (90 degrees)	chapter05::prerequisites
Basic	What is "projection"?	Mapping data onto lower dimensions	chapter05::prerequisites
Basic	Feature selection vs feature extraction?	Selection keeps originals; extraction creates new features	chapter05::dimensionality-reduction
Basic	What is PCA?	Principal Component Analysis	chapter05::pca
Basic	What is PCA's goal?	Find directions of maximum variance	chapter05::pca
Basic	Is PCA supervised or unsupervised?	Unsupervised	chapter05::pca
Basic	What are principal components?	Orthogonal axes of maximum variance	chapter05::pca
Cloze	PCA transformation: z = {{c1::xW}}		chapter05::pca
Basic	Step 1 of PCA?	Standardize the data	chapter05::pca-steps
Basic	Step 2 of PCA?	Compute covariance matrix	chapter05::pca-steps
Basic	Step 3 of PCA?	Get eigenvalues and eigenvectors	chapter05::pca-steps
Basic	Step 4 of PCA?	Sort eigenvalues descending	chapter05::pca-steps
Basic	Step 5 of PCA?	Select top k eigenvectors	chapter05::pca-steps
Basic	Step 6 of PCA?	Build projection matrix W	chapter05::pca-steps
Basic	Step 7 of PCA?	Transform data: z = xW	chapter05::pca-steps
Basic	Why standardize before PCA?	PCA is sensitive to scale	chapter05::pca
Basic	What do eigenvalues represent in PCA?	Variance explained by each component	chapter05::eigenvalues
Basic	Which eigenvector has most variance?	One with largest eigenvalue	chapter05::eigenvalues
Basic	What does positive covariance mean?	Features increase together	chapter05::covariance-matrix
Basic	What does negative covariance mean?	Features vary oppositely	chapter05::covariance-matrix
Cloze	Variance ratio = {{c1::λ_j / Σλ}}		chapter05::variance-explained
Basic	Why are PCs mutually orthogonal?	Property of eigendecomposition	chapter05::pca
Basic	What are factor loadings?	Correlations between features and PCs	chapter05::pca
Basic	How to choose number of PCs?	Cumulative variance ≥ 95%	chapter05::pca
Basic	What is a scree plot?	Variance per component, descending	chapter05::pca
Basic	What is LDA?	Linear Discriminant Analysis	chapter05::lda
Basic	What is LDA's goal?	Maximize class separability	chapter05::lda
Basic	Is LDA supervised or unsupervised?	Supervised	chapter05::lda
Basic	Key difference: PCA vs LDA?	PCA maximizes variance; LDA maximizes class separation	chapter05::pca-vs-lda
Basic	Step 1 of LDA?	Standardize the data	chapter05::lda-steps
Basic	Step 2 of LDA?	Compute mean vector per class	chapter05::lda-steps
Basic	Step 3 of LDA?	Compute S_B and S_W matrices	chapter05::lda-steps
Basic	Step 4 of LDA?	Get eigenvectors of S_W^(-1)S_B	chapter05::lda-steps
Basic	What is S_W in LDA?	Within-class scatter matrix	chapter05::lda
Basic	What is S_B in LDA?	Between-class scatter matrix	chapter05::lda
Cloze	LDA produces at most {{c1::c-1}} discriminants (c = classes)		chapter05::lda
Basic	Why max c-1 discriminants in LDA?	S_B has at most c-1 nonzero eigenvalues	chapter05::lda
Basic	LDA assumption 1?	Data is normally distributed	chapter05::lda
Basic	LDA assumption 2?	Classes have same covariance	chapter05::lda
Basic	LDA assumption 3?	Examples are independent	chapter05::lda
Basic	When might PCA beat LDA?	Few training examples per class	chapter05::pca-vs-lda
Basic	What is t-SNE used for?	Visualizing high-dim data in 2D/3D	chapter05::tsne
Basic	Key limitation of t-SNE?	Cannot project new data points	chapter05::tsne
Basic	t-SNE hyperparameters to tune?	Perplexity and learning rate	chapter05::tsne
Basic	What is UMAP?	Faster t-SNE alternative	chapter05::nonlinear
Basic	UMAP advantage over t-SNE?	Can project new data	chapter05::nonlinear
Basic	What is the curse of dimensionality?	High dims cause sparse data, overfitting	chapter05::dimensionality-reduction
Basic	What is manifold learning?	Nonlinear dimensionality reduction	chapter05::nonlinear
Basic	Write code to import PCA.	from sklearn.decomposition import PCA	chapter05::code
Basic	Write code to create PCA with 2 components.	pca = PCA(n_components=2)	chapter05::code
Basic	Write code to fit and transform with PCA.	X_pca = pca.fit_transform(X_std)	chapter05::code
Basic	How to get explained variance ratio?	pca.explained_variance_ratio_	chapter05::code
Basic	Write code to import LDA.	from sklearn.discriminant_analysis import LinearDiscriminantAnalysis	chapter05::code
Basic	Write code to create LDA with 2 components.	lda = LinearDiscriminantAnalysis(n_components=2)	chapter05::code
Basic	Write code to fit LDA (needs y).	X_lda = lda.fit_transform(X_std, y)	chapter05::code
Basic	Write code to compute covariance matrix.	cov_mat = np.cov(X_std.T)	chapter05::code
Basic	Write code for eigendecomposition.	eigenvalues, eigenvectors = np.linalg.eigh(cov_mat)	chapter05::code
Basic	Why use np.linalg.eigh over eig?	More stable for symmetric matrices	chapter05::code
Basic	Write code to import t-SNE.	from sklearn.manifold import TSNE	chapter05::code
Basic	Write code to run t-SNE.	X_tsne = TSNE(n_components=2, perplexity=30).fit_transform(X)	chapter05::code
Basic	Write code to get cumulative variance.	np.cumsum(pca.explained_variance_ratio_)	chapter05::code
