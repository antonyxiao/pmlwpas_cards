Type	Front	Back	Tags
Basic	What is the difference between feature selection and feature extraction?	Feature selection maintains original features; feature extraction transforms/projects data onto a new feature space	chapter05::dimensionality-reduction
Basic	What is the main goal of PCA?	To find directions of maximum variance in high-dimensional data and project onto a lower-dimensional subspace	chapter05::pca
Basic	What is the key difference between PCA and LDA?	PCA is unsupervised and maximizes variance; LDA is supervised and maximizes class separability	chapter05::pca-vs-lda
Cloze	In PCA, the orthogonal axes of the new subspace are called {{c1::principal components}}		chapter05::pca
Cloze	The PCA transformation can be written as {{c1::z = xW}}		chapter05::pca
Basic	What is step 1 of PCA?	Standardize the d-dimensional dataset	chapter05::pca-steps
Basic	What is step 2 of PCA?	Construct the covariance matrix	chapter05::pca-steps
Basic	What is step 3 of PCA?	Decompose the covariance matrix into eigenvectors and eigenvalues	chapter05::pca-steps
Basic	What is step 4 of PCA?	Sort eigenvalues by decreasing order to rank eigenvectors	chapter05::pca-steps
Basic	What is step 5 of PCA?	Select k eigenvectors corresponding to k largest eigenvalues	chapter05::pca-steps
Basic	What is step 6 of PCA?	Construct projection matrix W from top k eigenvectors	chapter05::pca-steps
Basic	What is step 7 of PCA?	Transform dataset X using projection matrix W	chapter05::pca-steps
Basic	Why must features be standardized before applying PCA?	PCA is sensitive to data scaling; standardization ensures all features have equal importance	chapter05::pca
Cloze	In PCA, eigenvalues represent the {{c1::amount of variance}} explained by each principal component		chapter05::eigenvalues
Cloze	PCA projects data onto {{c1::eigenvectors}} of the covariance matrix		chapter05::pca
Basic	What is eigendecomposition in PCA context?	Factorizing the covariance matrix into eigenvalues and eigenvectors	chapter05::eigenvalues
Cloze	The covariance matrix is a {{c1::symmetric}} matrix		chapter05::covariance-matrix
Cloze	When decomposing a symmetric matrix, eigenvalues are {{c1::real}} and eigenvectors are {{c2::orthogonal}}		chapter05::eigenvalues
Cloze	The eigenvector with the {{c1::highest eigenvalue}} corresponds to the direction of {{c2::maximum variance}}		chapter05::eigenvalues
Basic	What does a positive covariance between two features indicate?	The features increase or decrease together (vary in same direction)	chapter05::covariance-matrix
Basic	What does a negative covariance between two features indicate?	The features vary in opposite directions	chapter05::covariance-matrix
Cloze	The variance explained ratio of eigenvalue lambda_j is: {{c1::lambda_j / sum(all eigenvalues)}}		chapter05::variance-explained
Basic	Why are principal components mutually orthogonal?	This is a mathematical property of eigendecomposition of symmetric matrices	chapter05::pca
Cloze	In NumPy, {{c1::np.linalg.eigh()}} is more numerically stable for symmetric matrices than np.linalg.eig()		chapter05::pca-implementation
Basic	What are factor loadings in PCA?	Correlations between original features and principal components, computed by scaling eigenvectors by sqrt(eigenvalues)	chapter05::pca
Basic	What is the goal of LDA?	To find the feature subspace that optimizes class separability	chapter05::lda
Cloze	LDA is a {{c1::supervised}} dimensionality reduction technique		chapter05::pca-vs-lda
Basic	What is step 1 of LDA?	Standardize the d-dimensional dataset	chapter05::lda-steps
Basic	What is step 2 of LDA?	For each class, compute the d-dimensional mean vector	chapter05::lda-steps
Basic	What is step 3 of LDA?	Construct between-class scatter matrix S_B and within-class scatter matrix S_W	chapter05::lda-steps
Basic	What is step 4 of LDA?	Compute eigenvectors and eigenvalues of matrix S_W^(-1) * S_B	chapter05::lda-steps
Cloze	In LDA, the maximum number of linear discriminants is {{c1::c - 1}} where c is the number of classes		chapter05::lda
Basic	What is assumption 1 of LDA?	Data is normally distributed	chapter05::lda
Basic	What is assumption 2 of LDA?	Classes have identical covariance matrices	chapter05::lda
Basic	What is assumption 3 of LDA?	Training examples are statistically independent	chapter05::lda
Cloze	The within-class scatter matrix S_W is calculated by {{c1::summing individual scatter matrices of each class}}		chapter05::lda
Cloze	The between-class scatter matrix S_B measures the {{c1::scatter of class means around the overall mean}}		chapter05::lda
Basic	When might PCA outperform LDA for classification?	When each class has only a small number of training examples	chapter05::pca-vs-lda
Cloze	In scikit-learn, PCA is in {{c1::sklearn.decomposition.PCA}}		chapter05::implementation
Cloze	In scikit-learn, LDA is in {{c1::sklearn.discriminant_analysis.LinearDiscriminantAnalysis}}		chapter05::implementation
Basic	What does the explained_variance_ratio_ attribute in sklearn's PCA contain?	The proportion of variance explained by each principal component	chapter05::pca
Basic	Why might linear dimensionality reduction be unsuitable for some datasets?	When data has nonlinear relationships or lies on a curved manifold	chapter05::nonlinear
Basic	What is manifold learning?	Nonlinear dimensionality reduction techniques that capture structure of data on lower-dimensional manifolds	chapter05::nonlinear
Basic	What is t-SNE primarily used for?	Visualizing high-dimensional datasets in 2D or 3D by preserving local neighborhood structure	chapter05::tsne
Basic	What is a key limitation of t-SNE compared to PCA?	t-SNE cannot be applied to new data points; PCA creates a reusable transformation matrix	chapter05::tsne
Cloze	When using t-SNE, set init='{{c1::pca}}' for better preservation of global structure		chapter05::tsne
Basic	What hyperparameters should be tuned for t-SNE?	Perplexity (related to number of neighbors) and learning rate	chapter05::tsne
Basic	What is UMAP?	Uniform Manifold Approximation and Projection - similar to t-SNE but faster and can project new data	chapter05::nonlinear
Cloze	The first principal component always has the {{c1::largest possible variance}} among all projections		chapter05::pca
Basic	What is the curse of dimensionality?	Problems arising with high-dimensional data: sparse data, overfitting	chapter05::dimensionality-reduction
Basic	How do you determine how many principal components to keep?	Examine cumulative explained variance ratio; choose k components capturing sufficient percentage (e.g., 95%)	chapter05::pca
Basic	What does a scree plot show in PCA?	Variance explained by each principal component in descending order	chapter05::pca
Basic	Why does LDA produce at most c-1 linear discriminants?	The between-class scatter matrix has at most c-1 nonzero eigenvalues	chapter05::lda
Cloze	Feature extraction can improve performance by reducing the {{c1::curse of dimensionality}}		chapter05::dimensionality-reduction
Basic	Who formulated Fisher's Linear Discriminant?	R.A. Fisher in 1936 (original two-class version)	chapter05::lda
Basic	Who generalized LDA to multiple classes?	C.R. Rao in 1948	chapter05::lda
