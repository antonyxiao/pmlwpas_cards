Type	Front	Back	Tags
Basic	What is a "DAG"?	Directed Acyclic Graph	chapter13::prerequisites
Basic	What is "Xavier initialization"?	Weight init balancing gradient variance	chapter13::prerequisites
Basic	What is "embedding"?	Dense vector representation	chapter13::prerequisites
Basic	PyTorch graph type?	Directed acyclic graph (DAG)	chapter13::computation-graphs
Basic	Which dtypes can require gradients?	Floating point and complex	chapter13::autograd
Basic	What does trailing underscore mean?	In-place operation (e.g., method_())	chapter13::pytorch-conventions
Basic	What is Xavier/Glorot init for?	Balance gradient variance across layers	chapter13::initialization
Basic	PyTorch autograd implements?	Chain rule for gradients	chapter13::autograd
Basic	How to access gradients?	tensor.grad after backward()	chapter13::autograd
Basic	Forward vs reverse autodiff?	Reverse (PyTorch) is more efficient for backprop	chapter13::autograd
Basic	What is nn.Sequential?	Chain layers where output feeds next input	chapter13::nn-sequential
Basic	How to implement L2 regularization?	weight_decay parameter in optimizer	chapter13::regularization
Basic	How to implement L1 regularization?	Manually add to loss: weight.abs().sum()	chapter13::regularization
Basic	Loss for binary classification?	nn.BCELoss()	chapter13::loss-functions
Basic	Loss for multiclass?	nn.CrossEntropyLoss()	chapter13::loss-functions
Basic	Loss for regression?	nn.MSELoss()	chapter13::loss-functions
Basic	Why does Adam often beat SGD?	Adapts learning rate per parameter	chapter13::optimizers
Basic	What does XOR problem show?	Need hidden layers for nonlinear boundaries	chapter13::model-capacity
Basic	What is model capacity?	Ability to approximate complex functions	chapter13::model-capacity
Basic	Universal approximation theorem says?	One hidden layer can approximate any function	chapter13::model-capacity
Basic	Deeper vs wider networks tradeoff?	Deeper needs fewer params but harder to train	chapter13::model-capacity
Basic	Store layers for parameter registration in?	nn.ModuleList	chapter13::custom-layers
Basic	Make tensor trainable with?	nn.Parameter(tensor)	chapter13::custom-layers
Basic	Standard training loop order?	forward → loss → backward → step → zero_grad	chapter13::training-loop
Basic	Get scalar from loss tensor?	loss.item()	chapter13::training-loop
Basic	Two ways to handle categorical features?	One-hot encoding or nn.Embedding	chapter13::data-preprocessing
Basic	What is nn.Embedding?	Maps indices to trainable dense vectors	chapter13::data-preprocessing
Basic	What is nn.Flatten?	Flattens except batch dimension	chapter13::nn-layers
Basic	Sigmoid vs Softmax output?	Sigmoid for binary; Softmax for multiclass	chapter13::activation-functions
Basic	What is PyTorch Lightning?	Library reducing boilerplate code	chapter13::pytorch-lightning
Basic	Lightning models inherit from?	pl.LightningModule	chapter13::pytorch-lightning
Basic	What are adversarial examples?	Small perturbations causing misclassification	chapter13::advanced-topics
Basic	Write code for Xavier init.	nn.init.xavier_normal_(layer.weight)	chapter13::code
Basic	Write code to create Sequential model.	nn.Sequential(nn.Linear(10, 5), nn.ReLU(), nn.Linear(5, 1))	chapter13::code
Basic	Write code for L2 regularization.	optim.Adam(params, lr=0.001, weight_decay=0.01)	chapter13::code
Basic	Write code for BCELoss.	nn.BCELoss()	chapter13::code
Basic	Write code for one-hot encoding.	torch.nn.functional.one_hot(tensor, num_classes)	chapter13::code
Basic	Write code to create Embedding.	nn.Embedding(num_embeddings, embedding_dim)	chapter13::code
Basic	Write code for ModuleList.	nn.ModuleList([nn.Linear(10, 5), nn.Linear(5, 1)])	chapter13::code
Basic	Write code to get loss as float.	loss.item()	chapter13::code
Basic	Write code for ReLU layer.	nn.ReLU()	chapter13::code
Basic	Write code for Sigmoid layer.	nn.Sigmoid()	chapter13::code
Basic	Write code for Flatten layer.	nn.Flatten()	chapter13::code
