Type	Front	Back	Tags
Basic	What is a neuron (in biology)?	A nerve cell that processes signals	chapter02::prerequisites
Basic	What is a weight in ML?	A parameter that scales input importance	chapter02::prerequisites
Basic	What is a bias term?	A parameter that shifts the decision boundary	chapter02::prerequisites
Basic	What is a threshold?	A cutoff value for making decisions	chapter02::prerequisites
Basic	What is a derivative?	Rate of change of a function	chapter02::prerequisites
Basic	What is a gradient?	Vector of partial derivatives	chapter02::prerequisites
Basic	What does "convergence" mean?	When an algorithm reaches a stable solution	chapter02::prerequisites
Basic	What is the MCP neuron?	First mathematical model of a neuron (1943)	chapter02::history
Basic	What did the MCP neuron model?	A logic gate with binary outputs	chapter02::history
Basic	Who proposed the perceptron?	Frank Rosenblatt (1957)	chapter02::perceptron
Basic	What is a perceptron?	Single-layer neural network for binary classification	chapter02::perceptron
Cloze	Perceptron net input: z = {{c1::w·x + b}}		chapter02::perceptron
Basic	What does the perceptron output if z >= 0?	1	chapter02::perceptron
Basic	What does the perceptron output if z < 0?	0	chapter02::perceptron
Basic	What is the bias unit in a perceptron?	Negative threshold added to net input	chapter02::perceptron
Cloze	Perceptron weight update: w_j := w_j + {{c1::η(y - ŷ)x_j}}		chapter02::perceptron
Cloze	Perceptron bias update: b := b + {{c1::η(y - ŷ)}}		chapter02::perceptron
Basic	When do perceptron weights stay unchanged?	When prediction is correct (y = ŷ)	chapter02::perceptron
Basic	How does perceptron adjust for false negative?	Increases weights (makes z more positive)	chapter02::perceptron
Basic	How does perceptron adjust for false positive?	Decreases weights (makes z more negative)	chapter02::perceptron
Basic	What is η (eta)?	Learning rate	chapter02::learning-rate
Cloze	Learning rate is typically between {{c1::0}} and {{c2::1}}		chapter02::learning-rate
Basic	What is "linearly separable" data?	Data separable by a straight line/hyperplane	chapter02::convergence
Basic	When does perceptron converge?	Only if data is linearly separable	chapter02::convergence
Basic	How to handle non-separable data with perceptron?	Set maximum epochs or error threshold	chapter02::perceptron
Basic	Why not initialize weights to all zeros?	Learning rate only affects scale, not direction	chapter02::perceptron
Basic	What is an epoch?	One pass through entire training dataset	chapter02::training
Basic	What is Adaline?	ADAptive LInear NEuron	chapter02::adaline
Basic	How does Adaline differ from perceptron?	Uses continuous activation for weight updates	chapter02::adaline
Basic	What activation does Adaline use?	Identity function: σ(z) = z	chapter02::adaline
Basic	Why is continuous activation useful?	Makes loss differentiable for optimization	chapter02::adaline
Basic	What loss function does Adaline use?	Mean Squared Error (MSE)	chapter02::adaline
Cloze	MSE = {{c1::(1/n)Σ(y - ŷ)²}}		chapter02::adaline
Basic	Why is MSE useful for optimization?	It's differentiable and convex	chapter02::adaline
Basic	What is a convex function?	Has single global minimum, no local minima	chapter02::gradient-descent
Basic	What is gradient descent?	Optimization by moving opposite to gradient	chapter02::gradient-descent
Basic	Why move opposite to the gradient?	Gradient points uphill; we want downhill	chapter02::gradient-descent
Cloze	Gradient descent update: w := w - {{c1::η∇L}}		chapter02::gradient-descent
Basic	What determines step size in gradient descent?	Learning rate × gradient magnitude	chapter02::gradient-descent
Basic	What is batch gradient descent?	Update weights using ALL training examples	chapter02::gradient-descent
Basic	What if learning rate is too large?	Overshoots minimum, loss may increase	chapter02::learning-rate
Basic	What if learning rate is too small?	Very slow convergence	chapter02::learning-rate
Basic	What is feature standardization?	Transform to zero mean, unit variance	chapter02::standardization
Cloze	Standardization: x' = {{c1::(x - μ) / σ}}		chapter02::standardization
Basic	Why standardize features?	Similar scales help single learning rate work	chapter02::standardization
Basic	What is SGD?	Stochastic Gradient Descent	chapter02::sgd
Basic	How does SGD update weights?	After each individual example	chapter02::sgd
Basic	Name an advantage of SGD over batch.	Faster convergence, more frequent updates	chapter02::sgd
Basic	Another advantage of SGD?	Can escape shallow local minima	chapter02::sgd
Basic	Why shuffle data in SGD?	Prevents cycles in training	chapter02::sgd
Basic	What is online learning?	Training on-the-fly as new data arrives	chapter02::sgd
Basic	What is mini-batch gradient descent?	Update using small subsets (e.g., 32 examples)	chapter02::gradient-descent
Basic	Where does mini-batch fit between batch and SGD?	Compromise: more stable than SGD, faster than batch	chapter02::gradient-descent
Basic	What is an adaptive learning rate?	Learning rate that decreases during training	chapter02::sgd
Basic	What is One-versus-All (OvA)?	Train one classifier per class vs. rest	chapter02::classification
Cloze	OvA is also called {{c1::One-versus-Rest (OvR)}}		chapter02::classification
Basic	What is vectorization?	Replace loops with matrix operations	chapter02::implementation
Basic	Why use vectorization?	Much faster computation	chapter02::implementation
Basic	What does underscore suffix (w_) mean in sklearn?	Attribute set by fit(), not __init__	chapter02::implementation
Basic	What is an objective function?	Function to optimize during training	chapter02::training
Basic	How does Adaline make final predictions?	Threshold at 0.5 on activation output	chapter02::adaline
Basic	What does errors_ track in perceptron?	Misclassifications per epoch	chapter02::implementation
Basic	What does losses_ track in Adaline?	MSE loss per epoch	chapter02::implementation
Basic	What does partial_fit do?	Update weights without reinitializing	chapter02::sgd
Basic	Write code to compute net input.	z = np.dot(X, w) + b	chapter02::code
Basic	Write perceptron prediction code.	np.where(z >= 0, 1, 0)	chapter02::code
Basic	Write Adaline activation code.	return z  # identity function	chapter02::code
Basic	Write code to initialize weights randomly.	w = np.random.normal(0, 0.01, n_features)	chapter02::code
Basic	Write code to compute MSE loss.	loss = ((y - output) ** 2).mean()	chapter02::code
Basic	Write gradient descent weight update.	w += eta * X.T.dot(y - output) / n	chapter02::code
Basic	Write code to standardize a feature.	x_std = (x - x.mean()) / x.std()	chapter02::code
Basic	Write code to shuffle training data.	idx = np.random.permutation(len(y)); X, y = X[idx], y[idx]	chapter02::code
Basic	How to update weights in SGD for one sample?	w += eta * (y_i - output_i) * x_i	chapter02::code
Basic	Write code to track errors per epoch.	errors_.append((y != predictions).sum())	chapter02::code
