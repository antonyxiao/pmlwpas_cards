Type	Front	Back	Tags
Basic	What is "attention" in NLP?	Mechanism to focus on relevant parts	chapter16::prerequisites
Basic	What is "self-attention"?	Attention within a single sequence	chapter16::prerequisites
Basic	What are Q, K, V?	Query, Key, Value matrices	chapter16::prerequisites
Basic	What is "pre-training"?	Training on large unlabeled data first	chapter16::prerequisites
Basic	What is "fine-tuning"?	Adapting pre-trained model to specific task	chapter16::prerequisites
Basic	RNN encoder-decoder limitation?	Compresses all info into single vector	chapter16::rnn-attention
Basic	What does attention allow?	Access all inputs with relevance weights	chapter16::rnn-attention
Basic	Self-attention vs RNN attention?	Self-attention is within one sequence	chapter16::self-attention
Basic	Three stages of self-attention?	Derive weights, softmax, weighted sum	chapter16::self-attention
Cloze	Query: q = {{c1::Uq × x}}		chapter16::scaled-attention
Cloze	Key: k = {{c1::Uk × x}}		chapter16::scaled-attention
Cloze	Value: v = {{c1::Uv × x}}		chapter16::scaled-attention
Basic	Why scale by 1/√dk?	Large dot products cause tiny softmax gradients	chapter16::scaled-attention
Basic	What is multi-head attention?	Multiple parallel attention operations	chapter16::multi-head
Basic	How combine multi-head outputs?	Concatenate and project	chapter16::multi-head
Cloze	Original transformer: {{c1::8}} heads, {{c2::6}} layers		chapter16::transformer-architecture
Basic	Two sublayers in encoder?	Multi-head attention + feedforward	chapter16::transformer-architecture
Basic	What is masked attention?	Hide future positions in decoder	chapter16::transformer-architecture
Basic	Why are positional encodings needed?	Attention is permutation-invariant	chapter16::positional-encoding
Basic	Why layer norm over batch norm?	No batch dependency, small batches OK	chapter16::layer-norm
Cloze	Self-attention path length: {{c1::O(1)}}		chapter16::attention
Cloze	RNN path length: {{c1::O(n)}}		chapter16::attention
Cloze	Self-attention complexity: {{c1::O(n²)}}		chapter16::attention
Cloze	Self-attention formula: softmax({{c1::QKᵀ/√dk}})V		chapter16::attention
Basic	What architecture does GPT use?	Decoder-only, left-to-right	chapter16::gpt
Cloze	GPT-3 has {{c1::175B}} parameters		chapter16::gpt
Basic	What is zero-shot learning?	No task-specific examples	chapter16::gpt
Basic	What is few-shot learning?	Few examples in prompt	chapter16::gpt
Basic	What does BERT stand for?	Bidirectional Encoder Representations from Transformers	chapter16::bert
Basic	What architecture does BERT use?	Encoder-only, bidirectional	chapter16::bert
Basic	BERT's two pre-training tasks?	Masked Language Modeling, Next-Sentence Prediction	chapter16::bert
Cloze	BERT MLM masks {{c1::15%}} of tokens		chapter16::bert
Basic	What is [CLS] token?	Classification token for prediction	chapter16::bert
Basic	What is [SEP] token?	Sentence separator	chapter16::bert
Basic	Why is BERT better for classification?	Bidirectional gives richer representations	chapter16::bert
Basic	What is BART?	Bidirectional encoder + autoregressive decoder	chapter16::bart
Basic	What is DistilBERT?	Smaller BERT via knowledge distillation	chapter16::bert-finetuning
Basic	What is attention_mask?	1 for real tokens, 0 for padding	chapter16::bert-finetuning
Basic	What is cross-attention?	Queries from decoder, K/V from encoder	chapter16::transformer-architecture
Basic	What is teacher forcing?	Decoder gets ground-truth during training	chapter16::transformer-architecture
Cloze	Transformer paper (2017): "{{c1::Attention Is All You Need}}"		chapter16::transformer-architecture
Basic	Write code to import transformers.	from transformers import BertModel, BertTokenizer	chapter16::code
Basic	Write code to load BERT tokenizer.	tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')	chapter16::code
Basic	Write code to load BERT model.	model = BertModel.from_pretrained('bert-base-uncased')	chapter16::code
Basic	Write code to tokenize text.	tokens = tokenizer(text, return_tensors='pt', padding=True)	chapter16::code
Basic	Write code for nn.MultiheadAttention.	nn.MultiheadAttention(embed_dim, num_heads)	chapter16::code
Basic	Write code for nn.TransformerEncoder.	nn.TransformerEncoder(encoder_layer, num_layers)	chapter16::code
