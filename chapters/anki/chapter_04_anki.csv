Type	Front	Back	Tags
Basic	What is data preprocessing?	Preparing raw data for ML algorithms	chapter04::preprocessing-fundamentals
Basic	What is "missing data"?	Absent values in the dataset	chapter04::prerequisites
Basic	What is "imputation"?	Filling in missing values	chapter04::prerequisites
Basic	What is a "categorical feature"?	Non-numeric data like colors or labels	chapter04::prerequisites
Basic	What is "multicollinearity"?	High correlation between features	chapter04::prerequisites
Basic	What is "data leakage"?	Test info accidentally used in training	chapter04::prerequisites
Basic	What is "sparsity" (in ML)?	Most values being zero	chapter04::prerequisites
Basic	Three main preprocessing tasks?	Handle missing data, encode categories, select features	chapter04::preprocessing-fundamentals
Cloze	Missing values are typically {{c1::NaN}} or {{c2::NULL}}		chapter04::missing-data
Basic	How to count missing values per column?	df.isnull().sum()	chapter04::missing-data
Basic	What does dropna(axis=0) do?	Drops rows with missing values	chapter04::missing-data
Basic	What does dropna(axis=1) do?	Drops columns with missing values	chapter04::missing-data
Basic	Downside of dropping rows?	May lose too many samples	chapter04::missing-data
Basic	Downside of dropping columns?	May lose valuable features	chapter04::missing-data
Basic	What is mean imputation?	Replace missing with column mean	chapter04::missing-data
Basic	When use 'most_frequent' imputation?	For categorical features	chapter04::missing-data
Basic	What is ordinal data?	Categories with inherent order	chapter04::categorical-encoding
Basic	Example of ordinal data?	T-shirt sizes: XL > L > M	chapter04::categorical-encoding
Basic	What is nominal data?	Categories without order	chapter04::categorical-encoding
Basic	Example of nominal data?	Colors: red, green, blue	chapter04::categorical-encoding
Basic	How to encode ordinal features?	Map to integers preserving order	chapter04::categorical-encoding
Basic	Why not use integers for nominal features?	Model assumes incorrect ordering	chapter04::categorical-encoding
Basic	What is one-hot encoding?	Binary column per category value	chapter04::categorical-encoding
Basic	What problem does one-hot encoding cause?	Multicollinearity	chapter04::categorical-encoding
Basic	How to fix one-hot multicollinearity?	Drop one column (drop_first=True)	chapter04::categorical-encoding
Basic	What is binary encoding?	Log2(K) features instead of K-1	chapter04::categorical-encoding
Basic	What does stratify do in train_test_split?	Preserves class proportions	chapter04::data-partitioning
Basic	Common split ratio for small datasets?	70:30 or 80:20	chapter04::data-partitioning
Basic	Common split ratio for large datasets?	90:10 or 99:1	chapter04::data-partitioning
Basic	Why is feature scaling important?	Most algorithms need similar scales	chapter04::feature-scaling
Basic	Which algorithms don't need scaling?	Decision trees and random forests	chapter04::feature-scaling
Cloze	Min-max normalization: {{c1::(x - x_min) / (x_max - x_min)}}		chapter04::feature-scaling
Cloze	Standardization: {{c1::(x - mean) / std}}		chapter04::feature-scaling
Basic	Min-max scaling output range?	[0, 1]	chapter04::feature-scaling
Basic	Standardization output properties?	Mean=0, std=1	chapter04::feature-scaling
Basic	When prefer standardization?	Data has outliers	chapter04::feature-scaling
Basic	When prefer min-max?	Need bounded values, no outliers	chapter04::feature-scaling
Basic	Why fit scaler only on training data?	Prevent data leakage	chapter04::feature-scaling
Basic	What scaler handles outliers best?	RobustScaler	chapter04::feature-scaling
Basic	Four solutions to overfitting?	More data, regularization, simpler model, fewer features	chapter04::overfitting
Basic	What is L1 regularization?	Penalty on sum of absolute weights	chapter04::regularization
Basic	What is L2 regularization?	Penalty on sum of squared weights	chapter04::regularization
Basic	What does L1 regularization produce?	Sparse weights (many zeros)	chapter04::regularization
Basic	Why does L1 create sparsity?	Diamond constraint has corners on axes	chapter04::regularization
Basic	How is L1 used for feature selection?	Zero weights indicate unimportant features	chapter04::regularization
Basic	What is C in sklearn regularization?	Inverse of regularization strength	chapter04::regularization
Basic	What is feature selection?	Choosing subset of original features	chapter04::feature-selection
Basic	What is feature extraction?	Creating new features from originals	chapter04::feature-selection
Basic	What is SBS?	Sequential Backward Selection	chapter04::feature-selection
Basic	How does SBS work?	Remove features one at a time	chapter04::feature-selection
Basic	What is a greedy algorithm?	Makes locally optimal choices	chapter04::feature-selection
Basic	How do random forests measure importance?	Average impurity decrease	chapter04::feature-selection
Basic	RF importance issue with correlated features?	One ranks high, other underestimated	chapter04::feature-selection
Basic	Write code to check missing values.	df.isnull().sum()	chapter04::code
Basic	Write code to drop rows with NaN.	df.dropna(axis=0)	chapter04::code
Basic	Write code to create SimpleImputer with mean.	from sklearn.impute import SimpleImputer; imp = SimpleImputer(strategy='mean')	chapter04::code
Basic	Write code to fit and transform with imputer.	X_imputed = imp.fit_transform(X_train)	chapter04::code
Basic	Write code to one-hot encode with pandas.	pd.get_dummies(df, drop_first=True)	chapter04::code
Basic	Write code to create OneHotEncoder.	from sklearn.preprocessing import OneHotEncoder; ohe = OneHotEncoder()	chapter04::code
Basic	Write code to create LabelEncoder.	from sklearn.preprocessing import LabelEncoder; le = LabelEncoder()	chapter04::code
Basic	Write code to encode labels.	y_encoded = le.fit_transform(y)	chapter04::code
Basic	Write code to create MinMaxScaler.	from sklearn.preprocessing import MinMaxScaler; mms = MinMaxScaler()	chapter04::code
Basic	Write code to create StandardScaler.	from sklearn.preprocessing import StandardScaler; sc = StandardScaler()	chapter04::code
Basic	Write code for L1 logistic regression.	LogisticRegression(penalty='l1', solver='liblinear')	chapter04::code
Basic	Write code to get feature importances.	rf.feature_importances_	chapter04::code
Basic	Write code to create ColumnTransformer.	from sklearn.compose import ColumnTransformer	chapter04::code
Basic	Write code to create SelectFromModel.	from sklearn.feature_selection import SelectFromModel	chapter04::code
Basic	Write code to create KNNImputer.	from sklearn.impute import KNNImputer; knn_imp = KNNImputer(n_neighbors=5)	chapter04::code
