Type	Front	Back	Tags
Basic	What is "sequential data"?	Data where order matters (not IID)	chapter15::prerequisites
Basic	What is "recurrence"?	Feeding output back as input	chapter15::prerequisites
Basic	What is a "hidden state"?	Internal memory of the network	chapter15::prerequisites
Basic	What is "BPTT"?	Backpropagation Through Time	chapter15::prerequisites
Basic	Sequential vs time series data?	Time series has time dimension; sequential is just ordered	chapter15::sequential-data
Basic	What is many-to-one modeling?	Sequence input → single output	chapter15::sequence-modeling
Basic	Example of many-to-one?	Sentiment analysis	chapter15::sequence-modeling
Basic	What is one-to-many modeling?	Single input → sequence output	chapter15::sequence-modeling
Basic	Example of one-to-many?	Image captioning	chapter15::sequence-modeling
Basic	What is many-to-many modeling?	Sequence input → sequence output	chapter15::sequence-modeling
Basic	Example of many-to-many?	Machine translation	chapter15::sequence-modeling
Basic	How do RNNs differ from feedforward?	RNNs have memory via recurrent connections	chapter15::vanilla-rnn
Cloze	RNN hidden state receives {{c1::x(t)}} and {{c2::h(t-1)}}		chapter15::vanilla-rnn
Basic	Three RNN weight matrices?	Wxh, Whh, Who	chapter15::vanilla-rnn
Basic	Why are RNN weights shared across time?	Handle variable-length sequences	chapter15::vanilla-rnn
Basic	Initial hidden state?	Zeros or small random values	chapter15::vanilla-rnn
Basic	Default RNN activation?	tanh	chapter15::pytorch-rnn
Basic	What causes vanishing gradients in RNNs?	|w| < 1 raised to power becomes tiny	chapter15::vanishing-gradients
Basic	What causes exploding gradients?	|w| > 1 raised to power becomes huge	chapter15::exploding-gradients
Basic	Three solutions to RNN gradient problems?	Gradient clipping, truncated BPTT, LSTM	chapter15::gradient-problems
Basic	What is gradient clipping?	Cap gradients at threshold	chapter15::gradient-clipping
Basic	What is truncated BPTT?	Limit backprop to N recent steps	chapter15::tbptt
Basic	Who invented LSTM (1997)?	Hochreiter and Schmidhuber	chapter15::lstm
Basic	LSTM's main purpose?	Model long-range dependencies	chapter15::lstm
Cloze	LSTM has {{c1::3}} gates: forget, input, output		chapter15::lstm
Basic	What is LSTM cell state?	Direct path for information flow	chapter15::lstm
Basic	Forget gate controls?	How much to retain from cell state	chapter15::lstm
Basic	Input gate controls?	How much new info to add	chapter15::lstm
Basic	Output gate controls?	What to output to hidden state	chapter15::lstm
Basic	What is GRU?	Simpler LSTM variant (2014), fewer params	chapter15::gru
Basic	What is nn.Embedding for?	Map tokens to dense vectors	chapter15::embeddings
Cloze	Embedding matrix shape: {{c1::(vocab_size, embedding_dim)}}		chapter15::embeddings
Basic	Two advantages of embeddings?	Lower dimensionality, learnable representations	chapter15::embeddings
Basic	What is bidirectional RNN?	Process sequence both directions	chapter15::bidirectional-rnn
Basic	How are bidirectional outputs combined?	Usually concatenated	chapter15::bidirectional-rnn
Basic	What is autoregression in text?	Predicted token becomes next input	chapter15::language-modeling
Basic	For many-to-one, which hidden state used?	Final time step	chapter15::sentiment-analysis
Basic	Write code to create RNN.	nn.RNN(input_size, hidden_size, batch_first=True)	chapter15::code
Basic	Write code to create LSTM.	nn.LSTM(input_size, hidden_size, batch_first=True)	chapter15::code
Basic	Write code to create GRU.	nn.GRU(input_size, hidden_size, batch_first=True)	chapter15::code
Basic	Write code for Embedding layer.	nn.Embedding(vocab_size, embed_dim)	chapter15::code
Basic	Write code for bidirectional LSTM.	nn.LSTM(input_size, hidden_size, bidirectional=True)	chapter15::code
Basic	Write code to stack RNN layers.	nn.LSTM(input_size, hidden_size, num_layers=2)	chapter15::code
Basic	LSTM returns what?	output, (h_n, c_n)	chapter15::code
Basic	Write code for gradient clipping.	torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)	chapter15::code
