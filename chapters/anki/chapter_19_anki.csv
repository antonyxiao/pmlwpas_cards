Type	Front	Back	Tags
Basic	What is an "agent" in RL?	Entity that learns and takes actions	chapter19::prerequisites
Basic	What is "environment" in RL?	Everything the agent interacts with	chapter19::prerequisites
Basic	What is a "state"?	Observation of the environment	chapter19::prerequisites
Basic	What is an "action"?	Decision made by the agent	chapter19::prerequisites
Basic	What is a "reward"?	Feedback signal for an action	chapter19::prerequisites
Basic	What is a "policy"?	Strategy mapping states to actions	chapter19::prerequisites
Basic	How does RL differ from supervised?	Learns from interaction, not labels	chapter19::rl-fundamentals
Basic	What is the reward signal?	Scalar feedback indicating performance	chapter19::rewards
Basic	What is exploration-exploitation tradeoff?	Try new vs use known good actions	chapter19::exploration
Basic	What is delayed reward?	Action now, reward later	chapter19::rl-fundamentals
Basic	What is an episode?	Sequence of interactions until terminal	chapter19::episodes
Cloze	{{c1::Markov property}}: next state depends only on current state and action		chapter19::mdp
Basic	What is an MDP?	Markov Decision Process	chapter19::mdp
Basic	Model-based vs model-free?	Model-based learns dynamics; model-free doesn't	chapter19::model-types
Basic	Two model-free approaches?	Monte Carlo, Temporal Difference	chapter19::model-types
Basic	Deterministic vs stochastic environment?	Deterministic: p(s'|s,a) is 0 or 1	chapter19::mdp
Basic	Episodic vs continuing tasks?	Episodic ends; continuing runs forever	chapter19::episodes
Cloze	Return: Gt = R + {{c1::γ}}R' + {{c2::γ²}}R'' + ...		chapter19::returns
Basic	What if γ = 0?	Only immediate reward matters	chapter19::discount-factor
Basic	What if γ = 1?	All future rewards equal weight	chapter19::discount-factor
Basic	What is the value function v(s)?	Expected return from state s	chapter19::value-function
Basic	What is action-value q(s,a)?	Expected return from s taking action a	chapter19::value-function
Basic	Reward vs return vs value?	Reward: immediate; Return: episode sum; Value: expectation	chapter19::value-function
Basic	What is the Bellman equation?	Recursive relationship between state values	chapter19::bellman
Basic	Why is DP impractical?	Needs full transition probabilities	chapter19::dynamic-programming
Basic	What is epsilon-greedy?	Random action with probability ε	chapter19::exploration
Basic	What is GPI?	Generalized Policy Iteration	chapter19::gpi
Basic	How does MC differ from DP?	MC learns from episodes, no dynamics needed	chapter19::monte-carlo
Basic	MC estimates value how?	Average returns across episodes	chapter19::monte-carlo
Basic	How does TD differ from MC?	TD updates before episode ends	chapter19::temporal-difference
Cloze	TD target: {{c1::R + γV(S')}}		chapter19::temporal-difference
Basic	What is bootstrapping?	Update estimates using other estimates	chapter19::temporal-difference
Basic	On-policy vs off-policy?	On: learn from same policy; Off: learn from different	chapter19::td-control
Basic	What is SARSA?	On-policy TD control	chapter19::sarsa
Cloze	SARSA stands for {{c1::State-Action-Reward-State-Action}}		chapter19::sarsa
Basic	What is Q-learning?	Off-policy TD control	chapter19::q-learning
Cloze	Q-learning uses {{c1::max Q(S', a)}} for update		chapter19::q-learning
Basic	Why is Q-learning off-policy?	Uses max regardless of action taken	chapter19::q-learning
Basic	What is tabular Q-learning?	Q-values in lookup table	chapter19::q-learning
Basic	What is OpenAI Gym?	RL environments toolkit	chapter19::implementation
Basic	Why tabular Q-learning limited?	Large/continuous state spaces	chapter19::deep-q-learning
Basic	What is DQN?	Deep Q-Network	chapter19::deep-q-learning
Basic	What is replay memory?	Buffer for random sampling of transitions	chapter19::replay-memory
Basic	Why use replay memory?	Break sequential correlations	chapter19::replay-memory
Cloze	DQN target: {{c1::r + γ max Q(s', a')}}		chapter19::deep-q-learning
Basic	DQN loss function?	MSE between predicted and target Q	chapter19::deep-q-learning
Basic	Epsilon schedule?	Start high, decay to minimum	chapter19::exploration
Basic	Policy gradient advantage?	Handle continuous actions, stochastic policies	chapter19::policy-gradient
Basic	Write code to import gym.	import gym	chapter19::code
Basic	Write code to create gym environment.	env = gym.make('CartPole-v1')	chapter19::code
Basic	Write code to reset environment.	state = env.reset()	chapter19::code
Basic	Write code to take action.	next_state, reward, done, info = env.step(action)	chapter19::code
Basic	Write code for epsilon-greedy action.	action = random_action if random() < epsilon else argmax(Q[state])	chapter19::code
Basic	Write code for Q-learning update.	Q[s, a] += alpha * (r + gamma * Q[s', :].max() - Q[s, a])	chapter19::code
Basic	Write code for replay buffer sample.	batch = random.sample(replay_buffer, batch_size)	chapter19::code
