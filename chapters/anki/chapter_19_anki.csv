Type	Front	Back	Tags
Basic	What distinguishes RL from supervised/unsupervised learning?	Learning by interaction with environment to maximize reward, not from labels or patterns	chapter19::rl-fundamentals
Basic	What is an agent in RL?	Entity that learns decisions and interacts with environment through actions	chapter19::rl-fundamentals
Basic	What is the environment in RL?	Everything outside the agent; provides rewards and observations	chapter19::rl-fundamentals
Basic	What is the reward signal?	Scalar feedback (positive or negative) indicating how well agent performed	chapter19::rewards
Basic	What is the exploration-exploitation tradeoff?	Balance between discovering new actions (exploration) and using learned knowledge (exploitation)	chapter19::exploration
Basic	What is the delayed effect in RL?	Action at time t may result in reward appearing many steps later	chapter19::rl-fundamentals
Cloze	Sequence of interactions is called an {{c1::episode}}		chapter19::episodes
Cloze	{{c1::Markov property}}: next state depends only on current state and action, not history		chapter19::mdp
Basic	What is an MDP?	Mathematical formalization of sequential decision-making with states, actions, rewards, transitions	chapter19::mdp
Basic	What is model-based vs model-free RL?	Model-based learns/requires dynamics; model-free learns directly from experience	chapter19::model-types
Basic	What are two model-free approaches?	Monte Carlo (MC) and Temporal Difference (TD) methods	chapter19::model-types
Cloze	Environment is {{c1::deterministic}} when p(s',r|s,a) is 0 or 1		chapter19::mdp
Cloze	Environment is {{c1::stochastic}} when p(s',r|s,a) has other values		chapter19::mdp
Basic	What is the difference between episodic and continuing tasks?	Episodic has terminal states (games); continuing runs infinitely (cleaning robot)	chapter19::episodes
Cloze	Return: G_t = R_{t+1} + {{c1::gamma}}*R_{t+2} + {{c2::gamma^2}}*R_{t+3} + ...		chapter19::returns
Basic	What happens when discount factor gamma = 0?	Agent only considers immediate reward (short-sighted)	chapter19::discount-factor
Basic	What happens when discount factor gamma = 1?	All future rewards valued equally (no discounting)	chapter19::discount-factor
Cloze	Return recursively: G_t = {{c1::r + gamma*G_{t+1}}}		chapter19::returns
Cloze	A {{c1::policy}} pi(a|s) determines the next action to take		chapter19::policy
Basic	What is the optimal policy?	Policy that yields the highest return	chapter19::policy
Basic	What is the value function v_pi(s)?	Expected return starting from state s following policy pi	chapter19::value-function
Cloze	Action-value function q_pi(s,a): expected return at state s, taking action a, then following pi		chapter19::value-function
Basic	What is the difference between reward, return, and value?	Reward: immediate feedback. Return: weighted sum for episode. Value: expectation over episodes	chapter19::value-function
Cloze	Bellman equation: v_pi(s) = sum_a pi(a|s) sum_{s',r} p(s',r|s,a)[r + {{c1::gamma*v_pi(s')}}]		chapter19::bellman
Basic	Why is the Bellman equation important?	Relates state values recursively; simplifies computation	chapter19::bellman
Basic	Why is dynamic programming impractical for most RL?	Requires full knowledge of transition probabilities	chapter19::dynamic-programming
Cloze	{{c1::Epsilon-greedy}} gives non-optimal actions small probability epsilon		chapter19::exploration
Basic	What is generalized policy iteration (GPI)?	Alternating between policy evaluation and policy improvement	chapter19::gpi
Basic	How does Monte Carlo differ from dynamic programming?	MC learns from simulated episodes without knowing dynamics	chapter19::monte-carlo
Cloze	MC estimates value by computing {{c1::average return}} across episodes		chapter19::monte-carlo
Basic	How does TD differ from Monte Carlo?	TD updates before episode end using bootstrapping; MC waits for complete episodes	chapter19::temporal-difference
Cloze	TD(0) update: V(S_t) <- V(S_t) + alpha * ({{c1::R + gamma*V(S_{t+1})}} - V(S_t))		chapter19::temporal-difference
Basic	What is bootstrapping in TD?	Using learned estimates to update other estimates	chapter19::temporal-difference
Basic	What is on-policy vs off-policy TD control?	On-policy (SARSA): uses same policy. Off-policy (Q-learning): updates based on best action	chapter19::td-control
Cloze	SARSA stands for {{c1::State-Action-Reward-State-Action}}		chapter19::sarsa
Cloze	SARSA update uses {{c1::Q(S_{t+1}, A_{t+1})}}		chapter19::sarsa
Cloze	Q-learning update uses {{c1::max_a Q(S_{t+1}, a)}}		chapter19::q-learning
Basic	Why is Q-learning off-policy?	Uses max Q-value regardless of action actually taken	chapter19::q-learning
Basic	What is tabular Q-learning?	Q-values stored in lookup table; only for small discrete spaces	chapter19::q-learning
Basic	What is OpenAI Gym?	Toolkit with predefined RL environments and unified framework	chapter19::implementation
Basic	Why is tabular Q-learning insufficient for complex problems?	Large/continuous state spaces; no generalization to unseen states	chapter19::deep-q-learning
Cloze	A {{c1::Deep Q-Network (DQN)}} uses neural network for action-value function		chapter19::deep-q-learning
Basic	What is replay memory?	Buffer storing transitions; random sampling breaks correlations for training	chapter19::replay-memory
Basic	Why is replay memory necessary for DQNs?	NN training assumes IID; replay breaks sequential correlations	chapter19::replay-memory
Cloze	DQN target for non-terminal state: {{c1::r + gamma * max_a' Q(s', a')}}		chapter19::deep-q-learning
Basic	What loss function trains DQNs?	Mean Squared Error between predicted and target Q-values	chapter19::deep-q-learning
Cloze	Epsilon typically starts {{c1::high}} and decays to {{c2::minimum value}}		chapter19::exploration
Basic	What advantage do policy gradient methods have?	Handle continuous actions naturally; can learn stochastic policies	chapter19::policy-gradient
